{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I5iv-CjeceIf"
   },
   "source": [
    "CNARS/P: A tool for predicting job types in survey narratives using BERT models   \n",
    "\n",
    "This code is to build a pipeline to train BERT models with labeled data and predict classes with unlabeled data for job type classifictions.\n",
    "This project was supported by a grant from the National Science Foundation: FW-HTF-P Understanding Gig Work and its Effects on Wellbeing over the Life Course in the United States: A Machine Learning Approach (PI - Dr. Joelle Abramowitz; Co-PI - Dr. Jinseok Kim; 2021-10 ~ 2023.9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ttbMwR_7-oP6"
   },
   "source": [
    "Authors: Jinseok Kim (University of Michigan) & Jenna Kim (University of Illinois at Urbana-Champaign)   \n",
    "Created: 2022/6/14  \n",
    "Last Modified: 2023/10/08  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_NP0gStlatRJ"
   },
   "source": [
    "Updates:  \n",
    "* Download pretrained model (& tokenizer) using transformers API and save into local directory  \n",
    "* Load pretrained model from local directory for training process  \n",
    "* Save the best trained model (with highest validation accuracy) to the same directory  \n",
    "* Load the best trained model locally for testing process  \n",
    "* Train once, use the best model as many as needed for testing (labeled data vs unlabeled data)   \n",
    "* Add different pretrained models: RoBERTa & BERTweet  \n",
    "* Remove the function for data size change with ratio  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IAaw-vl8OzWx"
   },
   "source": [
    "References:  \n",
    "* https://curiousily.com/posts/sentiment-analysis-with-bert-and-hugging-face-using-pytorch-and-python/  \n",
    "* https://www.youtube.com/watch?v=f-86-HcYYi8  \n",
    "* https://mccormickml.com/2019/07/22/BERT-fine-tuning/  \n",
    "* https://www.analyticsvidhya.com/blog/2021/12/multiclass-classification-using-transformers/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0jdls3LTd2EV"
   },
   "source": [
    "# 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YXZS0zvBe_-e"
   },
   "source": [
    "## 1-1. Install package and load libraires"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e9NHn7cleEe9"
   },
   "source": [
    "Install the transformers package from Hugging Face which is a pytorch interface for working with a BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vzsrzXe7c1Oy"
   },
   "outputs": [],
   "source": [
    "# transformer ver: 4.15.0\n",
    "#!pip install transformers==4.15.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_bXe5lYvoOL0"
   },
   "outputs": [],
   "source": [
    "# install PyTorch\n",
    "# Note: No need to install PyTorch if this notebook is running on the AWS Sagemaker with pytorch kernel\n",
    "\n",
    "#!pip install torch==1.5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VGgZ9871oOL1"
   },
   "outputs": [],
   "source": [
    "# Check if the packages are correctly installed\n",
    "#!pip list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UvYcGEY5k0sE"
   },
   "source": [
    "Load other libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V77AFJ2RjlJ2"
   },
   "outputs": [],
   "source": [
    "import timeit\n",
    "import transformers\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "from collections import defaultdict\n",
    "from textwrap import wrap\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tMmmPU2Ik3m2"
   },
   "outputs": [],
   "source": [
    "# Set up for plots and parameters\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "sns.set(style='darkgrid', palette='muted', font_scale=1.5)\n",
    "COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n",
    "sns.set_palette(sns.color_palette(COLORS_PALETTE))\n",
    "rcParams[\"figure.figsize\"] = (12, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NnWN2xAjr2XE"
   },
   "outputs": [],
   "source": [
    "# Hide warning messages from display\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "twm3tuiifF2y"
   },
   "source": [
    "## 1-2. Check GPU for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dLbta3Vnf3P7"
   },
   "source": [
    "Note: If you use Google Colab, before running the next cell, make sure that the runtime type is set to GPU by going to Runtime => Change runtime type => select GPU for Hardware accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yTFWMkGPoOLz",
    "outputId": "51db915c-201e-4224-98ca-de265cefcdcc"
   },
   "outputs": [],
   "source": [
    "# Check a version of CUDA\n",
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s9-5qcHJeaMk",
    "outputId": "9f8627e3-f2e5-448f-ac40-42f23a025ffe"
   },
   "outputs": [],
   "source": [
    "# Check if there's a GPU available\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are {:d} GPU(s) available.'.format(torch.cuda.device_count()))\n",
    "    print('We will use the GPU: ', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "    print('No GPU available, using the CPU instead.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zU6PuvWbAeTX",
    "outputId": "d8f418c3-d1e3-4830-fafb-65207043019a"
   },
   "outputs": [],
   "source": [
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m7tcnC2SoOL3",
    "outputId": "4705faa9-d191-4f8a-82d9-4bfb146ef7c4"
   },
   "outputs": [],
   "source": [
    "# check GPU memory and utilization\n",
    "!nvidia-smi\n",
    "\n",
    "# To check the GPU memory usage while the process is running\n",
    "# open a terminal in the directory (Go to New-> Terminal) and type the above code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yo6hOQqWoOL4"
   },
   "outputs": [],
   "source": [
    "# clear the occupied cuda memory for efficient use\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Kill a process in running if more GPU space is needed\n",
    "#!sudo kill -9 3320"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a5j__pEHeayH"
   },
   "source": [
    "# 2. Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7kiPOqDBoOL5"
   },
   "source": [
    "## 2-1. If you load data from Google Drive directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GCBTvaJueobV"
   },
   "outputs": [],
   "source": [
    "#import os\n",
    "#from google.colab import drive\n",
    "#drive.mount('/gdrive')\n",
    "#%cd /gdrive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xXBsMyaGgshf"
   },
   "source": [
    "When running the above code, you might be required to enter authorization code to connect to Google Drive folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yb2IMEMneqD2"
   },
   "outputs": [],
   "source": [
    "# Access the directory where a dataset is stored\n",
    "#os.listdir(\"/gdrive/My Drive/Colab Notebooks/LabelingProject\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-UehQ6UihKnR"
   },
   "outputs": [],
   "source": [
    "#path = \"/gdrive/My Drive/Colab Notebooks/LabelingProject\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RckuoDMBhy9c"
   },
   "outputs": [],
   "source": [
    "# Load the dataset into a pandas dataframe\n",
    "# IMDB dataset: similar to Medline data\n",
    "# Required for reducing the data size due to the GPU constraints \n",
    "#df_raw = pd.read_csv(os.path.join(path, \"IMDB Dataset.csv\"))  \n",
    "\n",
    "# CoLA dataset: one sentence per each instance\n",
    "#df1 = pd.read_csv(os.path.join(path, \"in_domain_train.tsv\"), delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
    "#df2 = pd.read_csv(os.path.join(path, \"in_domain_dev.tsv\"), delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
    "\n",
    "#print(df1.shape)\n",
    "#print(df2.shape)\n",
    "\n",
    "#df_raw = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "#print(df_raw.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HmcQPrIBoOL7"
   },
   "source": [
    "## 2-2. If you load data from AWS SageMaker or your local directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OB54atl5oOL7"
   },
   "outputs": [],
   "source": [
    "def load_data(filename, colname, record):\n",
    "    \n",
    "    \"\"\"\n",
    "    Read in input file and load data\n",
    "    \n",
    "    filename: csv file\n",
    "    record: text file to save summary\n",
    "\n",
    "    return: dataframe\n",
    "    \n",
    "    \"\"\"\n",
    "    ## 1. Read in data from csv file\n",
    "    #df = pd.read_csv(filename, encoding=\"utf-8\", engine='python')\n",
    "    \n",
    "    # If unicodedecode error appears, use one of below options\n",
    "    # 1. Save dataset in utf-8 format: open csv file (file->save as-> CSV UTF-8)\n",
    "    # 2. Change encoding to 'unicode-escape' \n",
    "    df = pd.read_csv(filename, encoding=\"unicode-escape\")\n",
    "    \n",
    "    ## 2. No of rows and columns & data view\n",
    "    print(\"No of Rows (Raw data): {}\".format(df.shape[0]), file=record)\n",
    "    print(\"No of Columns: {}\".format(df.shape[1]), file=record)    \n",
    "    print(\"No of Rows (Raw data): {}\".format(df.shape[0]))\n",
    "    print(\"No of Columns: {}\".format(df.shape[1]))\n",
    "\n",
    "    print(\"\\n<Data View (Raw data)>\\n{}\".format(df.head(15)), file=record)\n",
    "    print(\"\\n<Data View (Raw data)>\\n{}\".format(df.head(15)))\n",
    "    \n",
    "    ## 3. Replace null values in any rows\n",
    "    # 3-1. Identify columns with null values\n",
    "    print(\"\\nCheck if null value exists:\\n\")\n",
    "    print(df.info())\n",
    "    print(\"\\nCheck if null value exists:\\n\", file=record)\n",
    "    df.info(buf=record)\n",
    "\n",
    "    # 3-2. Replace empty values with numpy NaN\n",
    "    df = df.replace(r'^\\s*$', np.nan, regex=True)\n",
    "    \n",
    "    # 3-3. Drop rows if both text and category columns have null values\n",
    "    df.dropna(how='all', subset=['text', 'category'], inplace = True)\n",
    "\n",
    "    print(\"\\nNo of rows (After handling null values): {}\".format(df.shape[0]), file=record)\n",
    "    print(\"No of columns: {}\".format(df.shape[1]), file=record)  \n",
    "    print(\"\\nNo of rows (After handling null values): {}\".format(df.shape[0]))\n",
    "    print(\"No of columns: {}\".format(df.shape[1]))\n",
    "\n",
    "    print(\"\\n<Data View (Null handled)>\\n{}\".format(df.head(15)), file=record)\n",
    "    print(\"\\n<Data View (Null handled)>\\n{}\".format(df.head(15)))\n",
    "        \n",
    "    ## 4. Select columns for processing\n",
    "    # if unlabeled data, fill in with a proxy number\n",
    "    if 'label' not in df.columns:\n",
    "      df['label']=[111 for i in range(df.shape[0])]\n",
    "    \n",
    "    # Select columns based on given column name\n",
    "    if colname == \"text\":\n",
    "      df = df[['instanceid', 'text', 'label']]\n",
    "      df.rename({\"text\": \"sentence\"}, axis=1, inplace=True)\n",
    "    elif colname == \"category\":\n",
    "      df = df[['pmid', 'category', 'label']]\n",
    "      df['category_str'] = df[['category']].apply(lambda x : str(x))\n",
    "      df.rename({\"category_str\": \"sentence\"}, axis=1, inplace=True)\n",
    "    elif colname == \"mix\":\n",
    "      df['mix'] = df[['text','category']].apply(lambda x : '{} {}'.format(x[0],str(x[1])), axis=1)\n",
    "      df = df[['pmid', 'mix', 'pubtype']]\n",
    "      df.rename({\"mix\": \"sentence\"}, axis=1, inplace=True)\n",
    "    \n",
    "    ## 5. Check the data\n",
    "    print(\"\\n<Data View: Selected Input>\\n{}\".format(df.head()), file=record)\n",
    "    print(\"\\n<Data View: Selected Input>\\n{}\".format(df.head()))\n",
    "    \n",
    "    print('\\nClass Counts(label, row): Total', file=record)\n",
    "    print(df.label.value_counts(), file=record)  \n",
    "    print('\\nClass Counts(label, row): Total')\n",
    "    print(df.label.value_counts())\n",
    "     \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IVTACKnXqXxM"
   },
   "source": [
    "# 3. Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VzFLjPm9vVpp"
   },
   "source": [
    "## 3-1. Check the distribution of token length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G8CCdCXooOL8"
   },
   "outputs": [],
   "source": [
    "def token_distribution(df, tokenizer, record):\n",
    "    token_lens = []\n",
    "    long_tokens = []\n",
    "\n",
    "    # remove null values\n",
    "    df = df.dropna()\n",
    "    \n",
    "    for id, txt in zip(df.instanceid, df.sentence):\n",
    "        tokens = tokenizer.encode(txt, padding=True, truncation=True, max_length=512)\n",
    "        token_lens.append(len(tokens))\n",
    "    \n",
    "        # Check a sentence with extreme length\n",
    "        if len(tokens) > 150:\n",
    "            long_tokens.append((id, len(tokens)))   \n",
    "  \n",
    "    print(\"\\n********** Check if Long Sentence Exists **********\\n\", file=record)\n",
    "    print(\"\\n********** Check if Long Sentence Exists **********\\n\")\n",
    "\n",
    "    if len(long_tokens)>0:\n",
    "      print(long_tokens, file=record) \n",
    "      print(long_tokens) \n",
    "    else:\n",
    "      print(\"No long sentence (<=150 tokens)\", file=record)\n",
    "      print(\"No long sentence (<=150 tokens)\")\n",
    "    \n",
    "    print(\"\\nMin token:\", min(token_lens), file=record)\n",
    "    print(\"Max token:\", max(token_lens), file=record)\n",
    "    print(\"Avg token:\", round(sum(token_lens)/len(token_lens)), file=record)\n",
    "    print(\"\\nMin token:\", min(token_lens))\n",
    "    print(\"Max token:\", max(token_lens))\n",
    "    print(\"Avg token:\", round(sum(token_lens)/len(token_lens)))\n",
    "    \n",
    "    # plot the distribution\n",
    "    sns.displot(token_lens)\n",
    "    plt.xlim([0, max(token_lens)+10])\n",
    "    plt.xlabel(\"Token Count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SqBtWF8Mw3Hx"
   },
   "source": [
    "## 3-2. Create a PyTorch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t5_qP5fQw7p5"
   },
   "outputs": [],
   "source": [
    "class LabelDataset(Dataset):\n",
    "    def __init__(self, reviews, targets, tokenizer, max_len):\n",
    "        self.reviews = reviews\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        review = str(self.reviews[item])\n",
    "        review = \" \".join(review.split())\n",
    "        target = self.targets[item]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            review,\n",
    "            None,                                    # second parameter is needed for a task of sentence similarity\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_token_type_ids=True,  \n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt')\n",
    "\n",
    "        return {\n",
    "            'texts': review,\n",
    "            'input_ids': encoding['input_ids'].flatten(),            # flatten() reduce dimension: e.g., [1, 512] -> [512]\n",
    "            'token_type_ids': encoding['token_type_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'targets': torch.tensor(target, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_A18krOXzNar"
   },
   "source": [
    "## 3-3. Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mDmZE8qYoOL8"
   },
   "outputs": [],
   "source": [
    "def sample_data(X_train, y_train, sampling=0, sample_method='over'):\n",
    "    \"\"\"\n",
    "       Sampling input train data\n",
    "       \n",
    "       X_train: dataframe of X train data\n",
    "       y_train: datafram of y train data\n",
    "       sampling: indicator of sampling funtion is on or off\n",
    "       sample_method: method of sampling (oversampling or undersampling)\n",
    "       \n",
    "    \"\"\"\n",
    "    \n",
    "    from imblearn.over_sampling import RandomOverSampler\n",
    "    from imblearn.under_sampling import RandomUnderSampler\n",
    "    \n",
    "    if sampling:\n",
    "        if sample_method == 'over':\n",
    "            oversample = RandomOverSampler(random_state=42)\n",
    "            X_over, y_over = oversample.fit_resample(X_train, y_train)\n",
    "            print('\\n****** Data Sampling ******')\n",
    "            print('\\nOversampled Data (class, Rows):\\n{}'.format(y_over.value_counts()))\n",
    "            X_train_sam, y_train_sam = X_over, y_over\n",
    "            \n",
    "        elif sample_method == 'under':\n",
    "            undersample = RandomUnderSampler(random_state=42)\n",
    "            X_under, y_under = undersample.fit_resample(X_train, y_train)\n",
    "            print('\\n****** Data Sampling ******')\n",
    "            print('\\nUndersampled Data (class,Rows):\\n{}'.format(y_under.value_counts()))\n",
    "            X_train_sam, y_train_sam = X_under, y_under\n",
    "    else:\n",
    "        X_train_sam, y_train_sam = X_train, y_train \n",
    "        print('\\n****** Data Sampling ******')\n",
    "        print('\\nNo Sampling Performed\\n')\n",
    "    \n",
    "    return X_train_sam, y_train_sam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G7Cqg09z2cOM"
   },
   "source": [
    "## 3-4. Create a data loader & classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FKtOnpVT12oS"
   },
   "outputs": [],
   "source": [
    "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
    "    ds = LabelDataset(\n",
    "        reviews = df.sentence.to_numpy(),\n",
    "        targets = df.label.to_numpy(),\n",
    "        tokenizer = tokenizer,\n",
    "        max_len = max_len\n",
    "    )\n",
    "    \n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size = batch_size,\n",
    "        num_workers = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rUnxYxw7BFs7"
   },
   "outputs": [],
   "source": [
    "class LabelClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_classes, model_loaded):\n",
    "        super(LabelClassifier, self).__init__()\n",
    "        self.bert = model_loaded\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "        self.linear = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        bert_out = self.bert(\n",
    "            input_ids = input_ids,\n",
    "            attention_mask = attention_mask,\n",
    "            token_type_ids = token_type_ids)\n",
    "        output_dropout = self.dropout(bert_out.pooler_output)\n",
    "        output = self.linear(output_dropout)\n",
    "    \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x-pK1UKzVEtx"
   },
   "source": [
    "# 4. Training & Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hAi3CD_oeCkg"
   },
   "source": [
    "The BERT authors's recommendations for fine-tuning:  \n",
    "* Batch size: 16, 32  \n",
    "* Learning rate (Adam): 5e-5, 3e-5, 2e-5  \n",
    "* Number of epochs: 2, 3, 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nTHCPOsheA30"
   },
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model,\n",
    "    data_loader,\n",
    "    loss_fn,\n",
    "    optimizer,\n",
    "    device,\n",
    "    scheduler,\n",
    "    n_examples,\n",
    "    outfile):\n",
    "    \n",
    "    model = model.train()\n",
    "    \n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "\n",
    "    for d in data_loader:\n",
    "        input_ids = d[\"input_ids\"].to(device, dtype=torch.long)\n",
    "        attention_mask = d[\"attention_mask\"].to(device, dtype=torch.long)\n",
    "        token_type_ids = d[\"token_type_ids\"].to(device, dtype=torch.long)\n",
    "        targets = d[\"targets\"].to(device)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids = input_ids,\n",
    "            attention_mask = attention_mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "\n",
    "        # printout for checking the prediction & target\n",
    "        #print(\"Pred: \", preds)\n",
    "        #print(\"Target: \", targets)\n",
    "\n",
    "        correct_predictions += torch.sum(preds == targets)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "    print(\"Correct Prediction (Train): {} out of {}\".format(correct_predictions.int(), n_examples), file=outfile)\n",
    "    print(\"Correct Prediction (Train): {} out of {}\".format(correct_predictions.int(), n_examples))\n",
    "\n",
    "    return correct_predictions.double() / n_examples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mkBOEEtbhofC"
   },
   "outputs": [],
   "source": [
    "def eval_model(\n",
    "    model, \n",
    "    data_loader, \n",
    "    loss_fn, \n",
    "    device, \n",
    "    n_examples,\n",
    "    outfile):\n",
    "    \n",
    "    model = model.eval()\n",
    "\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for d in data_loader:\n",
    "            input_ids = d[\"input_ids\"].to(device, dtype=torch.long)\n",
    "            attention_mask = d[\"attention_mask\"].to(device, dtype=torch.long)\n",
    "            token_type_ids = d[\"token_type_ids\"].to(device, dtype=torch.long)\n",
    "            targets = d[\"targets\"].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids = input_ids,\n",
    "                attention_mask = attention_mask,\n",
    "                token_type_ids = token_type_ids\n",
    "                )\n",
    "            \n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "\n",
    "            correct_predictions += torch.sum(preds == targets)\n",
    "            losses.append(loss.item())\n",
    "    \n",
    "    print(\"Correct Prediction (Eval): {} out of {}\".format(correct_predictions.int(), n_examples), file=outfile)\n",
    "    print(\"Correct Prediction (Eval): {} out of {}\".format(correct_predictions.int(), n_examples))\n",
    "    \n",
    "    return correct_predictions.double()/n_examples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xHJomabfoOL-"
   },
   "outputs": [],
   "source": [
    "def plot_train_history(history, modelname):\n",
    "  \n",
    "  plt.figure(figsize=(8, 6))\n",
    "  \n",
    "  # detach to cpu and convert float\n",
    "  train_acc = [float(x.to(\"cpu\").numpy()) for x in history[\"train_acc\"]]\n",
    "  val_acc = [float(x.to(\"cpu\").numpy()) for x in history[\"val_acc\"]]\n",
    "  \n",
    "  # plot\n",
    "  plt.plot(train_acc, 'b-o', label=\"train accuracy\")\n",
    "  plt.plot(val_acc, 'r-o', label=\"validation accuracy\")\n",
    "  plt.title(\"Training History\")\n",
    "  plt.ylabel(\"Accuracy\")\n",
    "  plt.xlabel(\"Epoch\")\n",
    "  plt.legend(loc='lower right', fontsize=15)\n",
    "  plt.xticks(history[\"epoch\"])\n",
    "  plt.yticks(np.arange(0,1.2,step=0.1))\n",
    "  plt.ylim([0,1.02])\n",
    "\n",
    "  # save to file\n",
    "  #filename = \"trainingplot_\" + modelname + \".png\"\n",
    "  #plt.savefig(filename)\n",
    "\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nvYnWEDsjUKE"
   },
   "outputs": [],
   "source": [
    "def training_loop(epochs, \n",
    "                  modelname, \n",
    "                  model, \n",
    "                  train_data_loader, \n",
    "                  val_data_loader, \n",
    "                  loss_fn, \n",
    "                  optimizer, \n",
    "                  device, \n",
    "                  scheduler, \n",
    "                  n_train, \n",
    "                  n_val,\n",
    "                  model_file,\n",
    "                  record):\n",
    "    \n",
    "    print(\"\\n**** Model Name: \" + modelname + \" *****\", file=record)\n",
    "    print(\"\\n**** Model Name: \" + modelname + \" *****\")\n",
    "    \n",
    "    history = defaultdict(list)\n",
    "    best_accuracy = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(\"\\nEpoch {} / {}\".format(str(epoch + 1), str(epochs)), file=record)\n",
    "        print(\"-\" * 60, file=record)\n",
    "    \n",
    "        print(\"\\nEpoch {} / {}\".format(str(epoch + 1), str(epochs)))\n",
    "        print(\"-\" * 60)\n",
    "    \n",
    "        train_acc, train_loss = train_model(\n",
    "            model, \n",
    "            train_data_loader,\n",
    "            loss_fn,\n",
    "            optimizer,\n",
    "            device,\n",
    "            scheduler,\n",
    "            n_train,\n",
    "            outfile=record)\n",
    "    \n",
    "        print(\"Train Loss: {}, Accuracy: {}\\n\".format(train_loss, train_acc), file=record)\n",
    "        print(\"Train Loss: {}, Accuracy: {}\\n\".format(train_loss, train_acc))\n",
    "    \n",
    "        val_acc, val_loss = eval_model(\n",
    "            model,\n",
    "            val_data_loader,\n",
    "            loss_fn,\n",
    "            device,\n",
    "            n_val,\n",
    "            outfile=record)\n",
    "    \n",
    "        print(\"Validation Loss: {}, Accuracy: {}\".format(val_loss, val_acc), file=record)  \n",
    "        print(\"Validation Loss: {}, Accuracy: {}\".format(val_loss, val_acc))\n",
    "\n",
    "        # store model state while training\n",
    "        history[\"epoch\"].append(epoch)\n",
    "        history[\"train_acc\"].append(train_acc)\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"val_acc\"].append(val_acc)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        \n",
    "        # save the best performing model\n",
    "        if val_acc > best_accuracy:\n",
    "            if model_file:\n",
    "              torch.save(model.state_dict(), model_file)\n",
    "\n",
    "              # for checking only: model's state_dict \n",
    "              #print(\"\\nModel's state_dict:\\n\")\n",
    "              #for param_tensor in model.state_dict():\n",
    "              #  print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "            \n",
    "            best_accuracy = val_acc\n",
    "    \n",
    "    # Plot training & validation accuracy\n",
    "    plot_train_history(history, modelname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ERH77nypnuFa"
   },
   "source": [
    "# 5. Prediction & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_iJYc8yzn02l"
   },
   "outputs": [],
   "source": [
    "def get_predictions(model, data_loader):\n",
    "    \n",
    "    model = model.eval()\n",
    "    \n",
    "    review_texts = []\n",
    "    predictions = []\n",
    "    prediction_probs = []\n",
    "    real_values = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for d in data_loader:\n",
    "            texts = d[\"texts\"]\n",
    "            input_ids = d[\"input_ids\"].to(device, dtype=torch.long)\n",
    "            attention_mask = d[\"attention_mask\"].to(device, dtype=torch.long)\n",
    "            token_type_ids = d[\"token_type_ids\"].to(device, dtype=torch.long)\n",
    "            targets = d[\"targets\"].to(device)\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids = input_ids,\n",
    "                attention_mask = attention_mask,\n",
    "                token_type_ids = token_type_ids\n",
    "            )\n",
    "            \n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "            # Apply the softmax or sigmoid function to normalize the raw output(logits) to get probability for each class\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            \n",
    "            review_texts.extend(texts)\n",
    "            predictions.extend(preds)\n",
    "            prediction_probs.extend(probs)\n",
    "            real_values.extend(targets)\n",
    "\n",
    "    # move the data to cpu\n",
    "    predictions = torch.stack(predictions).cpu()\n",
    "    prediction_probs = torch.stack(prediction_probs).cpu().detach().numpy()\n",
    "    real_values = torch.stack(real_values).cpu()\n",
    "\n",
    "    return review_texts, predictions, prediction_probs, real_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XAxJl4eFoOL_"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(y_test, y_pred, record, eval_model=0):\n",
    "    \"\"\"\n",
    "      evaluate model performance\n",
    "      \n",
    "      y_test: y test data\n",
    "      y_pred: t prediction score\n",
    "      eval_model: indicator if this funtion is on or off\n",
    "      \n",
    "    \"\"\"\n",
    "    \n",
    "    if eval_model:\n",
    "        \n",
    "        print('\\n************** Model Evaluation **************', file=record)\n",
    "        print('\\n************** Model Evaluation **************')\n",
    "        \n",
    "        print('\\nConfusion Matrix:\\n', file=record)\n",
    "        print('\\nConfusion Matrix:\\n')\n",
    "        print(confusion_matrix(y_test, y_pred), file=record)\n",
    "        print(confusion_matrix(y_test, y_pred))\n",
    "        \n",
    "        print('\\nClassification Report:\\n', file=record)\n",
    "        print('\\nClassification Report:\\n')\n",
    "        print(classification_report(y_test, y_pred, digits=4), file=record)\n",
    "        print(classification_report(y_test, y_pred, digits=4)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TQD2oIhdoOL_"
   },
   "outputs": [],
   "source": [
    "def predict_proba(df_test, y_text, y_test, y_pred, y_pred_probs, n_class, proba_file, test_unlabel_on=0, proba_out=0):\n",
    "    \n",
    "    \"\"\"\n",
    "       Get probability of each class\n",
    "       \n",
    "       df_test: original X test data\n",
    "       y_text: text data sentence\n",
    "       y_test: original y values\n",
    "       y_pred: predicted y values\n",
    "       y_pred_probs: probability scores of prediction\n",
    "       n_class: number of label class\n",
    "       proba_file: output file of probability scores\n",
    "       test_unlabel_on: on(1) and off(0) for using unlabeled test data\n",
    "       proba_out: on (1) and off (0) for probability output\n",
    "       \n",
    "    \"\"\"\n",
    "    if proba_out:\n",
    "       \n",
    "        prob_dict = {}\n",
    "        for i in range(n_class):\n",
    "          key_str = 'proba_' + str(i)\n",
    "          value_str = y_pred_probs[:, i]\n",
    "          prob_dict[key_str] = value_str\n",
    "        \n",
    "        df_proba = pd.DataFrame(prob_dict)\n",
    "        \n",
    "        if test_unlabel_on:\n",
    "          y_test = df_test[\"label\"].replace(111, \"NA\")\n",
    "\n",
    "        df_pred = pd.DataFrame({'instanceid': df_test[\"instanceid\"],\n",
    "                                'input': y_text,\n",
    "                                'act': y_test,\n",
    "                                'pred': y_pred})\n",
    "        \n",
    "        df_result = pd.concat([df_pred, df_proba], axis=1)\n",
    "        \n",
    "        ## Save output\n",
    "        df_result.to_csv(proba_file, encoding='utf-8', header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_fS7AKG_dxpj"
   },
   "source": [
    "# 6. Incorporating into main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "diHV5aDleqFy"
   },
   "outputs": [],
   "source": [
    "def train_test_labeled(input_file, colname, sample_on, sample_type, max_len, \n",
    "                       batch_size, modelname, model_download_path, device,\n",
    "                       learning_rate, epochs, model_file_path, eval_on, \n",
    "                       proba_on, proba_file, result_file, record):\n",
    "    \n",
    "    \"\"\"\n",
    "       Training and testing using labeled data\n",
    "       \n",
    "       input_file: input file\n",
    "       colname: colume name for selection between title and abstract\n",
    "       sample_on: indicator of sampling on or off\n",
    "       sample_type: sample type to choose if sample_on is 1\n",
    "       model_method: name of classifier to be applied for model fitting\n",
    "       eval_on: indicator of model evaluation on or off\n",
    "       proba_file: name of output file of probability\n",
    "       result_file: name of output file of evaluation\n",
    "       \n",
    "    \"\"\"\n",
    "    \n",
    "    ## 1. Load data\n",
    "   \n",
    "    print(\"\\n************** Loading Data **************\\n\", file=record)\n",
    "    print(\"\\n************** Loading Data **************\\n\")\n",
    "    df = load_data(input_file, colname, record=record)\n",
    "    \n",
    "    # Number of label class\n",
    "    n_class = len(df['label'].value_counts().keys().tolist())\n",
    "    print(\"\\nNumber of label class: \", n_class, file=record)\n",
    "    print(\"\\nNumber of label class: \", n_class)\n",
    "\n",
    "    # Check the first instance of input data\n",
    "    print(\"First Sentence: \\n\", df.sentence[0], file=record)\n",
    "    print(\"First Sentence: \\n\", df.sentence[0])\n",
    "    \n",
    "    ## 2. Train and test split\n",
    "    \n",
    "    print(\"\\n************** Spliting Data **************\\n\", file=record)\n",
    "    print(\"\\n************** Spliting Data **************\\n\")\n",
    "    \n",
    "    df_train, df_test = train_test_split(df, test_size=0.2, random_state=42, stratify=df.label)\n",
    "    df_val, df_test = train_test_split(df_test, test_size=0.5, random_state=42, stratify=df_test.label)\n",
    "    \n",
    "    print(\"Train Data: {}\".format(df_train.shape), file=record)\n",
    "    print(\"Val Data: {}\".format(df_val.shape), file=record)\n",
    "    print(\"Test Data: {}\".format(df_test.shape), file=record)\n",
    "    \n",
    "    print(\"Train Data: {}\".format(df_train.shape))\n",
    "    print(\"Val Data: {}\".format(df_val.shape))\n",
    "    print(\"Test Data: {}\".format(df_test.shape))\n",
    "    \n",
    "    print('\\nClass Counts(label, row): Train', file=record)\n",
    "    print(df_train.label.value_counts(), file=record)\n",
    "    print('\\nClass Counts(label, row): Val', file=record)\n",
    "    print(df_val.label.value_counts(), file=record)\n",
    "    print('\\nClass Counts(label, row): Test', file=record)\n",
    "    print(df_test.label.value_counts(), file=record)\n",
    "    \n",
    "    print(\"\\nTest Data\")\n",
    "    print(df_test.head())\n",
    "    \n",
    "    # Reset index\n",
    "    df_train=df_train.reset_index(drop=True)\n",
    "    df_val=df_val.reset_index(drop=True)\n",
    "    df_test=df_test.reset_index(drop=True)\n",
    "    \n",
    "    print(\"\\n************** Processing Data **************\", file=record)\n",
    "    print(\"\\n************** Processing Data **************\")\n",
    "    print(\"Train Data: {}\".format(df_train.shape), file=record)\n",
    "    print(\"Val Data: {}\".format(df_val.shape), file=record)\n",
    "    print(\"Test Data: {}\".format(df_test.shape), file=record)\n",
    "    \n",
    "    print(\"Train Data: {}\".format(df_train.shape))\n",
    "    print(\"Val Data: {}\".format(df_val.shape))\n",
    "    print(\"Test Data: {}\".format(df_test.shape))\n",
    "    \n",
    "    print('\\nClass Counts(label, row): Train', file=record)\n",
    "    print(df_train.label.value_counts(), file=record)\n",
    "    print('\\nClass Counts(label, row): Val', file=record)\n",
    "    print(df_val.label.value_counts(), file=record)\n",
    "    print('\\nClass Counts(label, row): Test', file=record)\n",
    "    print(df_test.label.value_counts(), file=record)\n",
    "    \n",
    "    print(\"\\nTest Data\")\n",
    "    print(df_test.head())\n",
    "    \n",
    "    ## 4. Sampling\n",
    "    if sample_on:\n",
    "        X_train = df_train.iloc[:, :-1]\n",
    "        y_train = df_train.iloc[:, -1]\n",
    "    \n",
    "        # Sampling\n",
    "        X_train_samp, y_train_samp = sample_data(X_train, y_train, sampling=sample_on, sample_method=sample_type)\n",
    "    \n",
    "        print(y_train_samp.value_counts(), file=record)\n",
    "\n",
    "        # Combine x_train and y_train data\n",
    "        df_train_concat = pd.concat([X_train_samp, y_train_samp], axis=1)\n",
    "\n",
    "        print(df_train_concat.info())\n",
    "        print(df_train_concat.head())\n",
    "    \n",
    "        # replace train data with sampled data\n",
    "        df_train = df_train_concat\n",
    "        print(df_train.shape)\n",
    "    \n",
    "    ## 5. Load data\n",
    "    # Load downloaded pretrained tokenizer from local directory\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_download_path)\n",
    "\n",
    "    # Check token distribution for defining MAX_LEN value\n",
    "    print(\"\\n************** Token Distribution of Train Data **************\")\n",
    "    token_distribution(df_train, tokenizer, record=record)\n",
    "\n",
    "    train_data_loader = create_data_loader(df_train, tokenizer, max_len, batch_size)\n",
    "    val_data_loader = create_data_loader(df_val, tokenizer, max_len, batch_size)\n",
    "    test_data_loader = create_data_loader(df_test, tokenizer, max_len, batch_size)\n",
    "\n",
    "    ## 6. Model Training\n",
    "    print(\"\\n************** Training Model: \" + modelname + \" **************\", file=record)\n",
    "    print(\"\\n************** Training Model: \" + modelname + \" **************\")\n",
    "    \n",
    "    # Check training time\n",
    "    start_time = timeit.default_timer()\n",
    "    \n",
    "    n_train = len(df_train)    \n",
    "    n_val = len(df_val)\n",
    "    \n",
    "    # Load downloaded pretrained model from local directory and move it to GPU\n",
    "    model_loaded = AutoModel.from_pretrained(model_download_path) \n",
    "    model = LabelClassifier(n_class, model_loaded)\n",
    "    model = model.to(device)   \n",
    "    \n",
    "    # define Optimizer & scheduler & loss function\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate, correct_bias=False)\n",
    "    total_steps = len(train_data_loader) * epochs\n",
    "\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps = 0,\n",
    "        num_training_steps = total_steps)\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "    \n",
    "    # Loop training with epochs\n",
    "    training_loop(epochs, modelname, model, train_data_loader, val_data_loader, \n",
    "                  loss_fn, optimizer, device, scheduler, n_train, n_val, \n",
    "                  model_file=model_file_path, record=record)\n",
    "    \n",
    "    elapsed = timeit.default_timer() - start_time\n",
    "    print(\"\\nTraining Time ({} epochs): {}\".format(epochs, round(elapsed, 2)), file=record)\n",
    "    print(\"\\nTraining Time ({} epochs): {}\".format(epochs, round(elapsed,2)))\n",
    "    \n",
    "    ## 7. Prediction  \n",
    "    print(\"\\n************** Getting Predictions **************\", file=record)\n",
    "    print(\"\\n************** Getting Predictions **************\")\n",
    "\n",
    "    # Load the best trained model \n",
    "    model = LabelClassifier(n_class, model_loaded)\n",
    "    model.load_state_dict(torch.load(model_file_path))\n",
    "    model = model.to(device)   \n",
    "    \n",
    "    # Get predictions\n",
    "    y_text, y_pred, y_pred_probs, y_test = get_predictions(model, test_data_loader) \n",
    "    \n",
    "    ## 8. Evaluating model performance      \n",
    "    print(\"\\n************** Evaluating Model Performance **************\", file=record)\n",
    "    print(\"\\n************** Evaluating Model Pperformance **************\")    \n",
    "    evaluate_model(y_test, y_pred, record=record, eval_model=eval_on)\n",
    "        \n",
    "    ## 9. Probability prediction   \n",
    "    print(\"\\n************** Producing Probability File **************\", file=record)\n",
    "    print(\"\\n************** Creating Probability File **************\") \n",
    "    predict_proba(df_test, y_text, y_test, y_pred, y_pred_probs, n_class, proba_file=proba_file, test_unlabel_on=test_unlabel_on, proba_out=proba_on)\n",
    "    \n",
    "    print(\"\\nOutput file: '\" + result_file + \"' Created\", file=record)\n",
    "    print(\"\\nOutput file: '\" + result_file + \"' Created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "11bguZb5V0Id"
   },
   "outputs": [],
   "source": [
    "def test_unlabeled(test_file, colname, max_len, batch_size, device, n_class,\n",
    "                   model_download_path, modelname, model_file_path, proba_on, \n",
    "                   test_unlabel_on, result_file, proba_file, record):\n",
    "    \"\"\"\n",
    "       Predict label for unlabeled test data and get probability file\n",
    "       \n",
    "       test_file: unlabeled data for getting prediction\n",
    "       colname: colume name for feeding as input string\n",
    "       eval_on: indicator of model evaluation on or off\n",
    "       proba_file: name of output file of probability\n",
    "       result_file: name of output file of evaluation\n",
    "       n_class: number of label class\n",
    "       \n",
    "    \"\"\"\n",
    "\n",
    "    # Check testing time\n",
    "    start_time = timeit.default_timer()\n",
    "    \n",
    "    # Load downloaded tokenizer & model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_download_path)\n",
    "    model_loaded = AutoModel.from_pretrained(model_download_path)\n",
    "\n",
    "    ## 1. Load test data     \n",
    "    print(\"\\n************** Loading Unlabeled Test Data **************\", file=record)\n",
    "    print(\"\\n************** Loading Unlabeled Test Data **************\")\n",
    "    df_unlabel = load_data(test_file, colname, record=record)\n",
    "    test_data_loader = create_data_loader(df_unlabel, tokenizer, max_len, batch_size)\n",
    "    \n",
    "    ## 2. Load the best trained model with checkpoint \n",
    "    print(\"\\n************** Loading Best Trained Model **************\", file=record)\n",
    "    print(\"\\n************** Loading Best Trained Model **************\")\n",
    "    print(\"\\nModel Name: \" + modelname, file=record)\n",
    "    print(\"\\nModel Name: \" + modelname)\n",
    "\n",
    "    model = LabelClassifier(n_class, model_loaded)\n",
    "    model.load_state_dict(torch.load(model_file_path))\n",
    "    model = model.to(device)\n",
    "\n",
    "    # for checking only: model's state_dict \n",
    "    #print(\"\\nLoaded Model's state_dict:\\n\")\n",
    "    #for param_tensor in model.state_dict():\n",
    "    #  print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "\n",
    "    ## 3. Prediction  \n",
    "    print(\"\\n************** Getting Predictions **************\", file=record)\n",
    "    print(\"\\n************** Getting Predictions **************\")\n",
    "    y_text, y_pred, y_pred_probs, y_test = get_predictions(model, test_data_loader) \n",
    "    \n",
    "    ## 4. Evaluating model performance      \n",
    "    print(\"\\n************** No Evaluation Conducted **************\", file=record)\n",
    "    print(\"\\n************** No Evaluation Conducted **************\")\n",
    "        \n",
    "    ## 5. Probability prediction   \n",
    "    print(\"\\n************** Creating Probability File **************\", file=record)\n",
    "    print(\"\\n************** Creating Probability File **************\") \n",
    "    df_test=df_unlabel\n",
    "    predict_proba(df_test, y_text, y_test, y_pred, y_pred_probs, n_class, proba_file=proba_file, test_unlabel_on=test_unlabel_on, proba_out=proba_on)\n",
    "    \n",
    "\n",
    "    print(\"\\nOutput file: '\" + result_file + \"' Created\", file=record)\n",
    "    print(\"\\nOutput file: '\" + result_file + \"' Created\")\n",
    "\n",
    "    elapsed = timeit.default_timer() - start_time\n",
    "    print(\"\\nTesting Time: {}\".format(round(elapsed, 2)), file=record)\n",
    "    print(\"\\nTesting Time: {}\".format(round(elapsed,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NPUSq_hgXEOc"
   },
   "outputs": [],
   "source": [
    "def main(input_file, test_file, colname, sample_on, sample_type, max_len, \n",
    "         batch_size, modelname, model_download_path, device, n_class,\n",
    "         learning_rate, epochs, model_file_path, test_unlabel_on, eval_on, \n",
    "         proba_on, proba_file, result_file):\n",
    "    \n",
    "    ## 0. open result file for records\n",
    "    f=open(result_file, \"a\")\n",
    "\n",
    "    if test_unlabel_on==0: \n",
    "      train_test_labeled(input_file, colname, sample_on, sample_type, max_len, \n",
    "                         batch_size, modelname, model_download_path, device, \n",
    "                         learning_rate, epochs, model_file_path, eval_on, \n",
    "                         proba_on, proba_file, result_file, record=f)\n",
    "    \n",
    "    elif test_unlabel_on==1:\n",
    "      test_unlabeled(test_file, colname, max_len, batch_size, device, n_class,\n",
    "                     model_download_path, modelname, model_file_path, proba_on,\n",
    "                     test_unlabel_on, result_file, proba_file, record=f)\n",
    "    \n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g-ZwBgE5oOL_"
   },
   "source": [
    "# 7. Main Code: Set Parameters & Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1RxI-Z6FoOMA",
    "outputId": "22e44b9b-d038-48f7-b8b0-2b3b54fb4964"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if __name__== \"__main__\":\n",
    "    \n",
    "    #####################################################################################    \n",
    "    ############################### Set Parameter Values ################################\n",
    "    #####################################################################################\n",
    "\n",
    "    ###### 1. Input file & column name and test (unlabeled) data exists?\n",
    "    input_filename = \"multi_labeled.csv\"  \n",
    "    column_name = \"text\"                                         # 'text'; 'category'; 'mix' for text+category\n",
    "    \n",
    "    test_unlabel_on = 0                                          # 1 if unlabeled data exists for test; otherwise 0\n",
    "    \n",
    "    test_filename = \"multi_unlabeled.csv\"                          # filename if test_unlabel_on=1; otherwise None\n",
    "    num_class=5                                                  # only needed when test_unlabel_on=1\n",
    "       \n",
    "    ###### 2. Sampling applied?\n",
    "    sampling_on = 0                                             # 0 for no sampling; 1 for sampling\n",
    "    sampling_type = 'under'                                      # Use when sampling_on=1; 'over'(oversampling), 'under'(undersampling)\n",
    "\n",
    "    ###### 3. Which pretrained model to use?\n",
    "\n",
    "    ###### 3-1. Model name\n",
    "    pretrained_model_name = 'bert-base-cased'\n",
    "    #pretrained_model_name = 'roberta-base'\n",
    "    #pretrained_model_name = 'vinai/bertweet-base'\n",
    "\n",
    "    ###### 3-2. Download pretrained model?\n",
    "    internet_on = 0                                              # 1 for downloading via internet connection; 0 for loading locally   \n",
    "    \n",
    "    modelname_string = pretrained_model_name.split(\"/\")[-1] \n",
    "    pretrained_model_folder = \"./model_\" + modelname_string\n",
    "\n",
    "    ###### 4. Hyperparameters \n",
    "    MAX_LEN = 100                                                # 150 for title; 512 for abs (Maximum input size: 512 (BERT))\n",
    "    BATCH_SIZE = 16                                              # Batch size: 16 or 32\n",
    "    EPOCHS = 4                                                   # Number of epochs: 2,3,4\n",
    "    LEARNING_RATE = 5e-5                                         # Learning rate:5e-5, 3e-5, 2e-5\n",
    "\n",
    "    ###### 5. Evaluation & probability file\n",
    "    eval_on=1                                                    # 0 for no; 1 for yes (display confusion matrix/classification report)\n",
    "    proba_on=1                                                   # 0 for no; 1 for yes (probability output)     \n",
    "\n",
    "    ###### 6. Output filename suffix\n",
    "    if test_unlabel_on==1:\n",
    "      label_name = \"unlabeled\"\n",
    "    else:\n",
    "      label_name= \"labeled\"\n",
    "\n",
    "\n",
    "    ###############################################3#####################################    \n",
    "    ################################# Run Main Fuction ###############################\n",
    "    #####################################################################################\n",
    "\n",
    "    if internet_on:    \n",
    "      \n",
    "      # download files of pretrained tokenizer & model using Huggingface API\n",
    "      tokenizer_download = AutoTokenizer.from_pretrained(pretrained_model_name)  \n",
    "      model_download = AutoModel.from_pretrained(pretrained_model_name)\n",
    "\n",
    "      # save to local folder\n",
    "      os.makedirs(pretrained_model_folder, exist_ok = True)\n",
    "      tokenizer_download.save_pretrained(pretrained_model_folder)    \n",
    "      model_download.save_pretrained(pretrained_model_folder)\n",
    "\n",
    "    else:    \n",
    "\n",
    "      if sampling_on:\n",
    "        proba_file = \"result_all_\" + modelname_string + \"_\" + sampling_type + \"_\" + column_name + \"_\" + label_name + \".csv\"  \n",
    "        eval_file = \"eval_all_\" + modelname_string + \"_\" + sampling_type + \"_\" + column_name + \"_\" + label_name + \".txt\"\n",
    "        best_model_file = pretrained_model_folder + \"/bestmodel_\" + modelname_string + \"_\" + sampling_type + \"_\" + column_name + \".pt\"\n",
    "      else:\n",
    "        proba_file = \"result_all_\" + modelname_string + \"_\" + column_name + \"_\" + label_name + \".csv\"  \n",
    "        eval_file = \"eval_all_\" + modelname_string + \"_\" + column_name + \"_\" + label_name + \".txt\" \n",
    "        best_model_file = pretrained_model_folder + \"/bestmodel_\" + modelname_string + \"_\" + column_name + \".pt\"\n",
    "            \n",
    "      main(input_file=input_filename,\n",
    "           test_file=test_filename, \n",
    "           colname=column_name,\n",
    "           n_class=num_class,\n",
    "           sample_on=sampling_on, \n",
    "           sample_type=sampling_type,\n",
    "           max_len=MAX_LEN, \n",
    "           batch_size=BATCH_SIZE,\n",
    "           modelname=modelname_string,\n",
    "           device=device,\n",
    "           model_download_path=pretrained_model_folder,\n",
    "           learning_rate=LEARNING_RATE,\n",
    "           epochs=EPOCHS,\n",
    "           model_file_path=best_model_file, \n",
    "           test_unlabel_on=test_unlabel_on,\n",
    "           eval_on=eval_on, \n",
    "           proba_file=proba_file,\n",
    "           proba_on=proba_on,\n",
    "           result_file=eval_file)\n",
    "        \n",
    "    print(\"\\n************** Processing Completed **************\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. internet_on=1\n",
    "### Download all models -> local files\n",
    "\n",
    "### 2. internet_on=0\n",
    "### all the dowloaded models are saved in subfolders in the current directory wit model names included in folder name\n",
    "\n",
    "### ====================================================================================\n",
    "\n",
    "### 3. test_unlabel_on=0\n",
    "###    test_filename = \".csv\" (labeled data) => fine-tuning (train BERT model with my train dat => best model save)\n",
    "\n",
    "### 4. test_unlabel_on=1\n",
    "###    test_filename = \".csv\"(unlabel data for testing only with best model)\n",
    "### final output: summary file 2, proba file 2\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "MULTI_BERT_V3_2.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
